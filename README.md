# Supply Chain Orders ETL Pipeline with Delta Lake

## Introduction

This project builds an ETL (Extract, Transform, Load) pipeline using Delta Lake on the Databricks platform to process order history data for an online clothing brand. The project uses Python and SQL for data processing and analysis, leveraging Delta Lake features such as `MERGE` operations and `Time Travel` for improved data management and performance.

## Objectives

- Process and analyze order data for an online clothing brand.
- Utilize Delta Lake features to improve data performance and integrity.
- Create an optimized ETL pipeline for supply chain analysis and sales process optimization.
  
**Set up the environment**:
   - Create an account on [Databricks](https://databricks.com/).
   - Set up a cluster on Databricks and install necessary libraries, including Delta Lake, PySpark, and Pandas.
